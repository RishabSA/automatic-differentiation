#include "NeuralNetwork.hpp"

NeuralNetwork::NeuralNetwork(std::vector<std::pair<int, int>> l) {
    // Input: vector of layers, each of (in_dim, out_dim)

    for (auto i : l) {
        int in_dim = i.first;
        int out_dim = i.second;

        Matrix W = Matrix(in_dim, out_dim);
        W.randomInit();

        Matrix b = Matrix(1, out_dim);

        layers.emplace_back(W, b);
    }
};

std::vector<std::pair<Matrix, Matrix>>& NeuralNetwork::getLayers() {
    return layers;
}

const std::vector<std::pair<Matrix, Matrix>>& NeuralNetwork::getLayers() const {
    return layers;
}

void NeuralNetwork::addLayer(std::pair<int, int> l) {
    // Input: layer with (in_dim, out_dim)
    int in_dim = l.first;
    int out_dim = l.second;

    Matrix W = Matrix(in_dim, out_dim);
    W.randomInit();

    Matrix b = Matrix(1, out_dim);

    layers.emplace_back(W, b);
};

Matrix NeuralNetwork::forward(const Matrix& input) {
    Matrix output = input;

    for (auto& layer : layers) {
        Matrix& W = layer.first;
        Matrix& b = layer.second;

        output = matmul(output, W) + b;
    }

    return output;
};

void NeuralNetwork::optimizeLayerWeights(double learning_rate) {
    // Backpropagation and Gradient Descent for each parameter
    for (auto& layer : layers) {
        Matrix& W = layer.first;
        Matrix& b = layer.second;

        // Update W
        for (int i = 0; i < W.rows; i++) {
            for (int j = 0; j < W.cols; j++) {
                Var& weight_param = W.data[i][j];

                // Partial derivative of the Loss function with respect to the weight parameter
                double gradient = weight_param.getGrad();
                weight_param.setVal(weight_param.getVal() - learning_rate * gradient);
            }
        }

        // Update b
        for (int i = 0; i < b.rows; i++) {
            for (int j = 0; j < b.cols; j++) {
                Var& bias_param = b.data[i][j];

                // Partial derivative of the Loss function with respect to the bias parameter
                double gradient = bias_param.getGrad();
                bias_param.setVal(bias_param.getVal() - learning_rate * gradient);
            }
        }
    }
};

std::string NeuralNetwork::getNetworkArchitecture() const {
    if (layers.empty()) {
        return "[]";
    }

    std::string architecture = "[";
    architecture += std::to_string(layers[0].first.rows);

    for (const auto& layer : layers) {
        architecture += " -> ";
        architecture += std::to_string(layer.first.cols);
    }

    architecture += "]";
    return architecture;
};